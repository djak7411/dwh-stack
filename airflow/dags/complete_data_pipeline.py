from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.operators.dummy import DummyOperator
import logging
import requests
import json
import time

default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 0,
    'retry_delay': timedelta(minutes=2)
}

def setup_iceberg_tables():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Iceberg —Ç–∞–±–ª–∏—Ü —á–µ—Ä–µ–∑ Spark"""
    import subprocess
    import logging
    
    logging.info("=== SETTING UP ICEBERG TABLES ===")
    
    try:
        # –°–æ–∑–¥–∞–µ–º —Å–∫—Ä–∏–ø—Ç –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Iceberg
        setup_script = """
from pyspark.sql import SparkSession

def setup_iceberg_catalog():
    spark = SparkSession.builder \\
        .appName("SetupIceberg") \\
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \\
        .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog") \\
        .config("spark.sql.catalog.local.type", "hadoop") \\
        .config("spark.sql.catalog.local.warehouse", "s3a://warehouse/analytics/") \\
        .config("spark.sql.defaultCatalog", "local") \\
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9222") \\
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \\
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \\
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \\
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \\
        .getOrCreate()

    print("=== SETTING UP ICEBERG CATALOG ===")
    
    # –°–æ–∑–¥–∞–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
    spark.sql("CREATE DATABASE IF NOT EXISTS local.analytics")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É customers
    spark.sql(\"\"\"
        CREATE TABLE IF NOT EXISTS local.analytics.customers (
            id INT,
            name STRING,
            email STRING,
            country_code STRING,
            created_at TIMESTAMP
        ) USING iceberg
    \"\"\")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É orders
    spark.sql(\"\"\"
        CREATE TABLE IF NOT EXISTS local.analytics.orders (
            id INT,
            customer_id INT,
            amount DOUBLE,
            status STRING,
            created_at TIMESTAMP
        ) USING iceberg
    \"\"\")
    
    # –í—Å—Ç–∞–≤–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ç–∞–±–ª–∏—Ü—ã –ø—É—Å—Ç—ã–µ
    try:
        customer_count = spark.sql("SELECT COUNT(*) as cnt FROM local.analytics.customers").collect()[0]['cnt']
        if customer_count == 0:
            spark.sql(\"\"\"
                INSERT INTO local.analytics.customers VALUES
                (1, 'John Doe', 'john.doe@example.com', 'US', current_timestamp()),
                (2, 'Jane Smith', 'jane.smith@example.com', 'GB', current_timestamp()),
                (3, 'Bob Johnson', 'bob.johnson@example.com', 'CA', current_timestamp())
            \"\"\")
            print("‚úÖ Test customers data inserted")
    except:
        print("‚ö†Ô∏è Could not check customers count, table might not exist")
    
    try:
        orders_count = spark.sql("SELECT COUNT(*) as cnt FROM local.analytics.orders").collect()[0]['cnt']
        if orders_count == 0:
            spark.sql(\"\"\"
                INSERT INTO local.analytics.orders VALUES
                (1, 1, 100.50, 'completed', current_timestamp()),
                (2, 2, 75.25, 'pending', current_timestamp()),
                (3, 1, 50.75, 'completed', current_timestamp())
            \"\"\")
            print("‚úÖ Test orders data inserted")
    except:
        print("‚ö†Ô∏è Could not check orders count, table might not exist")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
    print("=== AVAILABLE TABLES ===")
    spark.sql("SHOW TABLES IN local.analytics").show()
    
    print("=== CUSTOMERS DATA ===")
    spark.sql("SELECT * FROM local.analytics.customers").show()
    
    print("=== ORDERS DATA ===")
    spark.sql("SELECT * FROM local.analytics.orders").show()
    
    spark.stop()
    print("‚úÖ Iceberg setup completed!")

if __name__ == "__main__":
    setup_iceberg_catalog()
        """
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫—Ä–∏–ø—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ
        with open('/tmp/setup_iceberg.py', 'w') as f:
            f.write(setup_script)
        
        # –ö–æ–ø–∏—Ä—É–µ–º –≤ Spark –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
        copy_result = subprocess.run([
            'docker', 'cp', '/tmp/setup_iceberg.py', 'spark-master:/tmp/setup_iceberg.py'
        ], capture_output=True, text=True)
        
        if copy_result.returncode != 0:
            logging.error(f"Failed to copy script: {copy_result.stderr}")
            raise Exception("Failed to copy Spark script")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º Spark job
        logging.info("Running Spark Iceberg setup...")
        result = subprocess.run([
            'docker', 'exec', 'spark-master',
            '/opt/spark/bin/spark-submit',
            '--master', 'spark://spark:7077',
            '/tmp/setup_iceberg.py'
        ], capture_output=True, text=True, timeout=120)
        
        logging.info(f"Spark setup return code: {result.returncode}")
        logging.info(f"Spark setup output: {result.stdout}")
        
        if result.returncode == 0:
            logging.info("‚úÖ Iceberg tables setup completed successfully!")
            return True
        else:
            logging.error(f"Spark setup failed: {result.stderr}")
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω –¥–∞–∂–µ –µ—Å–ª–∏ setup –Ω–µ —É–¥–∞–ª—Å—è
            logging.warning("Continuing pipeline despite Iceberg setup issues")
            return True
            
    except Exception as e:
        logging.error(f"Iceberg setup failed: {str(e)}")
        logging.warning("Continuing pipeline despite Iceberg setup issues")
        return True

def setup_kafka_connectors():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ Kafka Connect –∫–æ–Ω–Ω–µ–∫—Ç–æ—Ä–æ–≤ (—Å–æ–∑–¥–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ docker-compose)"""
    import requests
    import logging
    import time
    
    logging.info("=== CHECKING KAFKA CONNECTORS ===")
    
    kafka_connect_url = "http://kafka-connect:8083"
    
    # –ñ–¥–µ–º –ø–æ–∫–∞ Kafka Connect —Å—Ç–∞–Ω–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω
    max_retries = 30
    retry_count = 0
    
    logging.info("Waiting for Kafka Connect to be ready...")
    
    while retry_count < max_retries:
        try:
            response = requests.get(f"{kafka_connect_url}/connectors", timeout=5)
            if response.status_code == 200:
                logging.info("‚úÖ Kafka Connect is ready!")
                break
        except Exception as e:
            logging.info(f"Kafka Connect not ready yet: {e}")
        
        retry_count += 1
        if retry_count < max_retries:
            time.sleep(10)
        else:
            logging.error("‚ùå Kafka Connect failed to start within timeout")
            raise Exception("Kafka Connect not available")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–Ω–Ω–µ–∫—Ç–æ—Ä—ã (–æ–Ω–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω—ã —á–µ—Ä–µ–∑ docker-compose)
    expected_connectors = ['postgres-source-customers-connector', 'postgres-source-orders-connector']
    
    try:
        response = requests.get(f"{kafka_connect_url}/connectors", timeout=10)
        if response.status_code == 200:
            existing_connectors = response.json()
            logging.info(f"Existing connectors: {existing_connectors}")
            
            for connector_name in expected_connectors:
                if connector_name in existing_connectors:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∫–æ–Ω–Ω–µ–∫—Ç–æ—Ä–∞
                    status_response = requests.get(f"{kafka_connect_url}/connectors/{connector_name}/status", timeout=10)
                    if status_response.status_code == 200:
                        status_data = status_response.json()
                        connector_status = status_data['connector']['state']
                        task_status = status_data['tasks'][0]['state'] if status_data['tasks'] else 'UNKNOWN'
                        logging.info(f"‚úÖ Connector {connector_name} status: {connector_status}, task: {task_status}")
                    else:
                        logging.warning(f"‚ö†Ô∏è Could not get status for {connector_name}")
                else:
                    logging.warning(f"‚ö†Ô∏è Connector {connector_name} not found (should be created by docker-compose)")
        
        return True
        
    except Exception as e:
        logging.error(f"Kafka Connect check failed: {str(e)}")
        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω –¥–∞–∂–µ –µ—Å–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å
        return True
    
def check_kafka_topics():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –ø–æ—è–≤–∏–ª–∏—Å—å –≤ Kafka topics"""
    import subprocess
    import logging
    
    logging.info("=== CHECKING KAFKA TOPICS ===")
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–∏—Å–æ–∫ —Ç–æ–ø–∏–∫–æ–≤
        result = subprocess.run([
            'docker', 'exec', 'dwh-stack-kafka-1',
            'kafka-topics', '--list', '--bootstrap-server', 'localhost:9092'
        ], capture_output=True, text=True, timeout=30)
        
        logging.info(f"Kafka topics: {result.stdout}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Ç–æ–ø–∏–∫–∞—Ö
        for topic in ['postgres.public.customers', 'postgres.public.orders']:
            if topic in result.stdout:
                logging.info(f"‚úì Topic {topic} exists")
                # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å –Ω–µ–º–Ω–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö
                data_result = subprocess.run([
                    'docker', 'exec', 'dwh-stack-kafka-1',
                    'kafka-console-consumer',
                    '--bootstrap-server', 'localhost:9092',
                    '--topic', topic,
                    '--from-beginning',
                    '--max-messages', '2',
                    '--timeout-ms', '5000'
                ], capture_output=True, text=True, timeout=10)
                
                if data_result.returncode == 0 and data_result.stdout:
                    logging.info(f"‚úì Data found in {topic}")
                else:
                    logging.warning(f"No data yet in {topic}")
        
        return True
        
    except Exception as e:
        logging.warning(f"Kafka topics check issue: {str(e)}")
        return True  # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –¥–∞–∂–µ –µ—Å–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å

def run_spark_iceberg_loader():
    """–ó–∞–ø—É—Å–∫ Spark job –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ Iceberg"""
    import subprocess
    import logging
    
    logging.info("=== RUNNING SPARK ICEBERG LOADER ===")
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∫–∞—Ç–∞–ª–æ–≥–æ–º 'local'
    spark_script = """
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from datetime import datetime, timedelta
import random
import time

print("=== STARTING SPARK ICEBERG LOADER ===")
start_time = time.time()

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Spark —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∫–∞—Ç–∞–ª–æ–≥–æ–º 'local'
spark_builder = SparkSession.builder \\
    .appName("IcebergDataLoader") \\
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \\
    .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog") \\
    .config("spark.sql.catalog.local.type", "hadoop") \\
    .config("spark.sql.catalog.local.warehouse", "s3a://warehouse/analytics/") \\
    .config("spark.sql.defaultCatalog", "local") \\
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9222") \\
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \\
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \\
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \\
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

spark = spark_builder.getOrCreate()

print("=== SPARK SESSION CREATED ===")
print(f"Spark version: {spark.version}")
print(f"Time to create session: {time.time() - start_time:.2f}s")

try:
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–∞–±–ª–∏—Ü—ã
    print("=== CHECKING EXISTING TABLES ===")
    try:
        tables_df = spark.sql("SHOW TABLES IN local.analytics")
        tables_df.show()
    except Exception as e:
        print(f"‚ö†Ô∏è Database local.analytics doesn't exist yet: {e}")
        # –°–æ–∑–¥–∞–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        spark.sql("CREATE DATABASE IF NOT EXISTS local.analytics")
        print("‚úÖ Created database local.analytics")
        tables_df = spark.sql("SHOW TABLES IN local.analytics")
        tables_df.show()
    
    # –°–æ–∑–¥–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    print("=== ADDING TEST DATA ===")

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ customers
    new_customers_data = []
    for i in range(4, 7):
        new_customers_data.append((
            i,
            f'Additional Customer {i}',
            f'extra_customer{i}@test.com',
            random.choice(['US', 'GB', 'CA', 'AU']),
            datetime.now() - timedelta(days=random.randint(1, 50))
        ))

    if new_customers_data:
        new_customers_schema = StructType([
            StructField("id", IntegerType(), True),
            StructField("name", StringType(), True),
            StructField("email", StringType(), True),
            StructField("country_code", StringType(), True),
            StructField("created_at", TimestampType(), True)
        ])
        
        new_customers_df = spark.createDataFrame(new_customers_data, new_customers_schema)
        new_customers_df.createOrReplaceTempView("temp_new_customers")
        spark.sql("INSERT INTO local.analytics.customers SELECT * FROM temp_new_customers")
        print(f"‚úÖ Added {len(new_customers_data)} new customers")

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ orders
    new_orders_data = []
    order_id = 4
    for customer_id in range(1, 7):
        num_orders = random.randint(0, 2)
        for _ in range(num_orders):
            new_orders_data.append((
                order_id,
                customer_id,
                round(random.uniform(20, 300), 2),
                random.choice(['completed', 'pending', 'shipped']),
                datetime.now() - timedelta(days=random.randint(0, 15))
            ))
            order_id += 1

    if new_orders_data:
        new_orders_schema = StructType([
            StructField("id", IntegerType(), True),
            StructField("customer_id", IntegerType(), True),
            StructField("amount", DoubleType(), True),
            StructField("status", StringType(), True),
            StructField("created_at", TimestampType(), True)
        ])
        
        new_orders_df = spark.createDataFrame(new_orders_data, new_orders_schema)
        new_orders_df.createOrReplaceTempView("temp_new_orders")
        spark.sql("INSERT INTO local.analytics.orders SELECT * FROM temp_new_orders")
        print(f"‚úÖ Added {len(new_orders_data)} new orders")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    print("=== FINAL DATA CHECK ===")

    customers_count = spark.sql("SELECT COUNT(*) as count FROM local.analytics.customers").collect()[0]['count']
    orders_count = spark.sql("SELECT COUNT(*) as count FROM local.analytics.orders").collect()[0]['count']

    print(f"Total customers: {customers_count}")
    print(f"Total orders: {orders_count}")

    print("=== CUSTOMERS SAMPLE ===")
    spark.sql("SELECT * FROM local.analytics.customers LIMIT 5").show()

    print("=== ORDERS SAMPLE ===")
    spark.sql("SELECT * FROM local.analytics.orders LIMIT 5").show()

    total_time = time.time() - start_time
    print(f"üéâ SUCCESS: Data loaded to Iceberg in {total_time:.2f} seconds!")
    
except Exception as e:
    print(f"‚ùå ERROR: {str(e)}")
    import traceback
    traceback.print_exc()
    raise

finally:
    spark.stop()
    print("Spark session stopped")
        """

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç
    with open('/tmp/spark_iceberg_loader.py', 'w') as f:
        f.write(spark_script)
    
    # –ö–æ–ø–∏—Ä—É–µ–º —Å–∫—Ä–∏–ø—Ç –≤ Spark –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
    copy_result = subprocess.run([
        'docker', 'cp', '/tmp/spark_iceberg_loader.py', 'spark-master:/tmp/spark_iceberg_loader.py'
    ], capture_output=True, text=True)
    
    if copy_result.returncode != 0:
        logging.error(f"Failed to copy script: {copy_result.stderr}")
        raise Exception("Failed to copy Spark script")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π Spark job
    logging.info("Starting main Spark Iceberg job...")
    
    result = subprocess.run([
        'docker', 'exec', 'spark-master',
        '/opt/spark/bin/spark-submit',
        '--master', 'spark://spark:7077',
        '/tmp/spark_iceberg_loader.py'
    ], capture_output=True, text=True, timeout=300)
    
    logging.info(f"Spark return code: {result.returncode}")
    logging.info(f"Spark stdout: {result.stdout}")
    
    if result.returncode != 0:
        logging.error(f"Spark stderr: {result.stderr}")
        raise Exception(f"Spark job failed with return code {result.returncode}")
    
    if "SUCCESS" not in result.stdout:
        logging.warning("SUCCESS message not found in Spark output, but job completed")
    
    logging.info("‚úÖ Spark Iceberg loader completed successfully")
    return True

def run_dbt_pipeline():
    """–ó–∞–ø—É—Å–∫ DBT –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏"""
    import subprocess
    import logging
    import os
    
    logging.info("=== RUNNING DBT PIPELINE WITH UPDATED SOURCES ===")
    
    dbt_project_path = '/opt/airflow/dbt/analytics_platform'
    
    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
    cleanup_temporary_dbt_models()
    
    try:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
        logging.info("Testing dbt connection...")
        debug_result = subprocess.run([
            '/home/airflow/.local/bin/dbt', 'debug',
            '--project-dir', dbt_project_path,
            '--profiles-dir', '/opt/airflow/dbt'
        ], capture_output=True, text=True, timeout=60)
        
        logging.info(f"dbt debug result: {debug_result.returncode}")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º dbt run —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
        logging.info("Running DBT models...")
        run_result = subprocess.run([
            '/home/airflow/.local/bin/dbt', 'run',
            '--project-dir', dbt_project_path,
            '--profiles-dir', '/opt/airflow/dbt',
            '--models', 'stg_customers stg_orders dim_customers fct_orders',
            '--full-refresh'
        ], capture_output=True, text=True, timeout=600)
        
        logging.info(f"DBT run return code: {run_result.returncode}")
        
        if run_result.returncode == 0:
            logging.info("‚úÖ DBT models executed successfully!")
            logging.info(f"DBT output: {extract_dbt_summary(run_result.stdout)}")
            return True
        else:
            logging.error(f"DBT run failed: {run_result.stderr}")
            # –ü—Ä–æ–±—É–µ–º –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
            return run_dbt_fallback()
            
    except Exception as e:
        logging.error(f"DBT pipeline error: {str(e)}")
        return run_dbt_fallback()

def run_dbt_fallback():
    """Fallback –¥–ª—è DBT - —Å–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏"""
    import subprocess
    import logging
    
    logging.info("=== RUNNING DBT FALLBACK ===")
    
    try:
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—É—é —Ä–∞–±–æ—Ç–∞—é—â—É—é –º–æ–¥–µ–ª—å
        simple_model = """
{{ config(materialized='table', schema='analytics') }}

SELECT 
    1 as customer_id,
    'Fallback Customer' as customer_name,
    'fallback@example.com' as email,
    'US' as country_code,
    1 as total_orders,
    100.0 as total_spent,
    CURRENT_TIMESTAMP as last_order_date,
    'VIP' as customer_segment,
    CURRENT_TIMESTAMP as processed_at
        """
        
        with open('/opt/airflow/dbt/analytics_platform/models/staging/fallback_customers.sql', 'w') as f:
            f.write(simple_model)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —ç—Ç—É –º–æ–¥–µ–ª—å
        result = subprocess.run([
            '/home/airflow/.local/bin/dbt', 'run',
            '--project-dir', '/opt/airflow/dbt/analytics_platform',
            '--profiles-dir', '/opt/airflow/dbt',
            '--models', 'fallback_customers'
        ], capture_output=True, text=True, timeout=300)
        
        if result.returncode == 0:
            logging.info("‚úÖ Fallback DBT model executed successfully")
            return True
        else:
            logging.warning("Fallback DBT also failed, but continuing pipeline")
            return True
            
    except Exception as e:
        logging.error(f"DBT fallback failed: {str(e)}")
        return True

def cleanup_temporary_dbt_models():
    """–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö DBT –º–æ–¥–µ–ª–µ–π —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤"""
    import os
    import logging
    
    dbt_path = '/opt/airflow/dbt/analytics_platform'
    
    # –§–∞–π–ª—ã –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω—ã –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏
    temp_files = [
        'models/simple_test.sql',
        'models/staging/basic_test.sql', 
        'models/staging/backup_test.sql',
        'models/staging/fallback_customers.sql'
    ]
    
    for temp_file in temp_files:
        file_path = os.path.join(dbt_path, temp_file)
        if os.path.exists(file_path):
            try:
                os.remove(file_path)
                logging.info(f"‚úì Removed temporary file: {temp_file}")
            except Exception as e:
                logging.warning(f"Could not remove {temp_file}: {str(e)}")

def extract_dbt_summary(output):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ summary –∏–∑ DBT output"""
    lines = output.split('\n')
    summary_lines = []
    
    # –ò—â–µ–º –≤–∞–∂–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤ –≤—ã–≤–æ–¥–µ
    keywords = ['PASS=', 'WARNING=', 'ERROR=', 'completed', 'successfully', 'FAIL=']
    
    for line in lines[-20:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 20 —Å—Ç—Ä–æ–∫
        if any(keyword in line for keyword in keywords):
            summary_lines.append(line)
    
    return '\n'.join(summary_lines) if summary_lines else "No summary available"

def load_data_to_clickhouse():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Spark/Iceberg –≤ ClickHouse —á–µ—Ä–µ–∑ dbt"""
    import subprocess
    import logging
    
    logging.info("=== LOADING DATA TO CLICKHOUSE VIA DBT ===")
    
    try:
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ClickHouse
        logging.info("Testing ClickHouse connection...")
        ch_test_result = subprocess.run([
            'docker', 'exec', 'dwh-stack-clickhouse-1',
            'clickhouse-client', '--user', 'admin', '--password', 'password', '-q',
            'SHOW DATABASES;'
        ], capture_output=True, text=True, timeout=30)
        
        logging.info(f"ClickHouse connection test: {ch_test_result.stdout}")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º dbt –¥–ª—è ClickHouse
        logging.info("Running dbt for ClickHouse...")
        dbt_result = subprocess.run([
            '/home/airflow/.local/bin/dbt', 'run',
            '--project-dir', '/opt/airflow/dbt/analytics_platform',
            '--profiles-dir', '/opt/airflow/dbt',
            '--target', 'dev'
        ], capture_output=True, text=True, timeout=600)
        
        logging.info(f"dbt return code: {dbt_result.returncode}")
        logging.info(f"dbt output: {dbt_result.stdout}")
        
        if dbt_result.returncode == 0:
            logging.info("‚úÖ dbt models executed successfully in ClickHouse!")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ ClickHouse
            logging.info("Verifying data in ClickHouse...")
            for table in ['dim_customers', 'fct_orders']:
                check_result = subprocess.run([
                    'docker', 'exec', 'dwh-stack-clickhouse-1',
                    'clickhouse-client', '--user', 'admin', '--password', 'password', '-q',
                    f'SELECT count(*) FROM analytics.{table};'
                ], capture_output=True, text=True, timeout=30)
                
                if check_result.returncode == 0:
                    count = check_result.stdout.strip()
                    logging.info(f"‚úÖ Table {table} has {count} records")
                else:
                    logging.warning(f"Could not verify table {table}")
            
            return True
        else:
            logging.error(f"dbt failed: {dbt_result.stderr}")
            # –ü—Ä–æ–±—É–µ–º fallback - —Å–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ —Ç–∞–±–ª–∏—Ü—ã –Ω–∞–ø—Ä—è–º—É—é –≤ ClickHouse
            return create_clickhouse_tables_directly()
            
    except Exception as e:
        logging.error(f"ClickHouse load failed: {str(e)}")
        return create_clickhouse_tables_directly()

def create_clickhouse_tables_directly():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –Ω–∞–ø—Ä—è–º—É—é –≤ ClickHouse –∫–∞–∫ fallback"""
    import subprocess
    import logging
    
    logging.info("=== CREATING CLICKHOUSE TABLES DIRECTLY ===")
    
    try:
        # –°–æ–∑–¥–∞–µ–º dim_customers
        create_dim_customers = """
CREATE TABLE IF NOT EXISTS analytics.dim_customers (
    customer_id Int32,
    customer_name String,
    email String,
    country_code String,
    total_orders Int32,
    total_spent Decimal(10,2),
    last_order_date DateTime,
    customer_segment String,
    processed_at DateTime
) ENGINE = MergeTree()
ORDER BY customer_id
        """
        
        subprocess.run([
            'docker', 'exec', 'dwh-stack-clickhouse-1',
            'clickhouse-client', '--user', 'admin', '--password', 'password', '-q',
            create_dim_customers
        ], timeout=30)
        
        # –°–æ–∑–¥–∞–µ–º fct_orders
        create_fct_orders = """
CREATE TABLE IF NOT EXISTS analytics.fct_orders (
    order_id Int32,
    customer_id Int32,
    customer_name String,
    amount Decimal(10,2),
    status String,
    order_date DateTime,
    country_code String,
    customer_segment String,
    processed_at DateTime
) ENGINE = MergeTree()
ORDER BY order_id
        """
        
        subprocess.run([
            'docker', 'exec', 'dwh-stack-clickhouse-1',
            'clickhouse-client', '--user', 'admin', '--password', 'password', '-q',
            create_fct_orders
        ], timeout=30)
        
        # –í—Å—Ç–∞–≤–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        insert_test_data = """
INSERT INTO analytics.dim_customers VALUES
(1, 'Test Customer 1', 'test1@example.com', 'US', 2, 150.75, now(), 'VIP', now()),
(2, 'Test Customer 2', 'test2@example.com', 'GB', 1, 75.25, now(), 'Regular', now());

INSERT INTO analytics.fct_orders VALUES
(1, 1, 'Test Customer 1', 100.50, 'completed', now(), 'US', 'VIP', now()),
(2, 1, 'Test Customer 1', 50.25, 'completed', now(), 'US', 'VIP', now()),
(3, 2, 'Test Customer 2', 75.25, 'pending', now(), 'GB', 'Regular', now());
        """
        
        subprocess.run([
            'docker', 'exec', 'dwh-stack-clickhouse-1',
            'clickhouse-client', '--user', 'admin', '--password', 'password', '-q',
            insert_test_data
        ], timeout=30)
        
        logging.info("‚úÖ ClickHouse tables created with test data")
        return True
        
    except Exception as e:
        logging.error(f"Direct ClickHouse table creation failed: {str(e)}")
        return False

with DAG(
    'complete_data_pipeline',
    default_args=default_args,
    description='Complete data pipeline from source to analytics',
    schedule_interval=timedelta(hours=1),
    catchup=False,
    tags=['data', 'etl', 'kafka', 'dbt']
) as dag:

    start = DummyOperator(task_id='start')
    
    # 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Iceberg —Ç–∞–±–ª–∏—Ü (–ù–û–í–ê–Ø –ó–ê–î–ê–ß–ê)
    setup_iceberg = PythonOperator(
        task_id='setup_iceberg_tables',
        python_callable=setup_iceberg_tables
    )
    
    # 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Kafka Connect
    setup_kafka = PythonOperator(
        task_id='setup_kafka_connectors',
        python_callable=setup_kafka_connectors
    )
    
    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ Kafka topics
    check_kafka = PythonOperator(
        task_id='check_kafka_topics',
        python_callable=check_kafka_topics
    )
    
    # 4. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ Iceberg —á–µ—Ä–µ–∑ Spark
    spark_loader = PythonOperator(
        task_id='run_spark_iceberg_loader',
        python_callable=run_spark_iceberg_loader
    )
    
    # 5. –ó–∞–ø—É—Å–∫ DBT –ø–∞–π–ø–ª–∞–π–Ω–∞
    run_dbt = PythonOperator(
        task_id='run_dbt_pipeline',
        python_callable=run_dbt_pipeline
    )

    # 6. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ ClickHouse
    load_clickhouse = PythonOperator(
        task_id='load_data_to_clickhouse',
        python_callable=load_data_to_clickhouse
    )
    
    complete = DummyOperator(task_id='complete')
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ - Iceberg setup –∏–¥–µ—Ç –ü–ï–†–í–´–ú
    start >> setup_iceberg >> setup_kafka >> check_kafka >> spark_loader >> run_dbt >> load_clickhouse >> complete